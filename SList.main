#!/usr/bin/env python3
import hashlib, random, struct, time, csv, math
import pandas as pd
import argparse

N = 8192 

# Configs
SAMPLE_EVERY = 10
RANDOM_SEED = None
GAMMA_PROBABILITY = 0.5


def sha256(x: bytes) -> bytes:
    return hashlib.sha256(x).digest()

def canonical_bytes(arg):
    if isinstance(arg, int):
        data = b'\x00' if arg == 0 else arg.to_bytes((arg.bit_length() + 7) // 8, 'big')
        tag = b'I'
    elif isinstance(arg, str):
        data = arg.encode('utf-8')
        tag = b'S'
    elif isinstance(arg, bytes):
        data = arg
        tag = b'B'
    elif arg is None:
        data = b''
        tag = b'N'
    else:
        data = str(arg).encode('utf-8')
        tag = b'S'
    return tag + struct.pack('>I', len(data)) + data


class Node:
    __slots__ = ('level', 'key', 'value', 'down', 'right', 'rank', 'label', 'leaf_hash')
    def __init__(self, level, key=None, value=None):
        self.level = level
        self.key = key
        self.value = value
        self.down = None
        self.right = None
        self.rank = 0
        self.label = None
        self.leaf_hash = None


class AuthSkipList:
    def __init__(self, expected_size):
        self.max_level = math.ceil(math.log2(expected_size)) if expected_size > 0 else 8
        self.level_heads = [Node(l) for l in range(self.max_level + 1)]
        for l in range(self.max_level, 0, -1):
            self.level_heads[l].down = self.level_heads[l - 1]
        self.head = self.level_heads[self.max_level]
        self.count = 0
        self.actual_height = 0
        # Initialize all head labels to empty hash
        for head in self.level_heads:
            head.label = sha256(canonical_bytes(head.level) + canonical_bytes(0) +
                               canonical_bytes(b'') + canonical_bytes(b''))

    def _hash_leaf(self, key, value):
        return sha256(canonical_bytes(key) + canonical_bytes(value))

    def random_level(self):
        lvl = 0
        while random.random() < 0.5 and lvl < self.max_level:
            lvl += 1
        return lvl

    def _compute_rank(self, node):
        """Compute rank of a node from its down pointer."""
        if node.level == 0:
            return 1
        subtotal = 0
        d = node.down
        while d:
            subtotal += d.rank
            d = d.right
        return subtotal

    def _compute_label(self, node):
        """Compute label of a node from its children."""
        right_hash = node.right.label if (node.right and node.right.label) else b''
        if node.level == 0:
            child_hash = node.leaf_hash if node.leaf_hash else b''
        else:
            child_hash = node.down.label if (node.down and node.down.label) else b''
        ser = canonical_bytes(node.level) + canonical_bytes(node.rank) \
              + canonical_bytes(child_hash) + canonical_bytes(right_hash)
        return sha256(ser)

    def insert(self, key, value):
        prevs = [None] * (self.max_level + 1)
        node = self.head

        # Find insertion path
        for l in reversed(range(self.max_level + 1)):
            while node.right and node.right.key is not None and node.right.key < key:
                node = node.right
            prevs[l] = node
            if node.down:
                node = node.down

        lvl = self.random_level()
        if lvl > self.actual_height:
            self.actual_height = lvl

        down_link = None
        new_nodes = []
        for l in range(lvl + 1):
            new = Node(l, key, value)
            if l == 0:
                new.leaf_hash = self._hash_leaf(key, value)
                new.rank = 1
                new.label = new.leaf_hash
            new.down = down_link
            new.right = prevs[l].right
            prevs[l].right = new
            new_nodes.append(new)
            down_link = new
        self.count += 1

        self._update_path(prevs, new_nodes)

    def _update_path(self, prevs, new_nodes):
        """Update ranks and labels along the insert. path only."""

        # Update ranks bottom-up for new nodes
        for l in range(len(new_nodes)):
            new_nodes[l].rank = self._compute_rank(new_nodes[l])

        # Update ranks for nodes on the insert. path
        for l in range(len(prevs)):
            if prevs[l] is None:
                continue
            prevs[l].rank = self._compute_rank(prevs[l])

        # Must update labels from bottom to top so child labels are ready
        for l in range(self.actual_height + 1):

            if l < len(new_nodes):
                new_nodes[l].label = self._compute_label(new_nodes[l])

            # Update predecessor node label
            if l < len(prevs) and prevs[l]:
                prevs[l].label = self._compute_label(prevs[l])

            # Update head node at this level
            head = self.level_heads[l]
            head.rank = self._compute_rank(head)
            head.label = self._compute_label(head)

        # Update head nodes above actual_height up to max_level
        for l in range(self.actual_height + 1, self.max_level + 1):
            head = self.level_heads[l]
            head.rank = self._compute_rank(head)
            head.label = self._compute_label(head)

    def get_root(self):
        return self.head.label

    def get_actual_height(self):
        return self.actual_height

    def get_proof_chain(self, rank):
        if rank < 1 or rank > self.count:
            raise IndexError("rank out of range")

        proof = []
        gammas = []
        remaining = rank
        current = self.head

        for level in reversed(range(self.max_level + 1)):
            while current.right and current.right.rank < remaining:
                remaining -= current.right.rank
                current = current.right
            
            # OPTION 2 ENHANCED: Add multiple hashes per level to make computation clearly O(log n)
            # Hash the current node's rank multiple times (simulates expensive crypto operations)
            h = sha256(canonical_bytes(current.rank))
            for _ in range(4):  # 5 total hashes per level
                h = sha256(h + canonical_bytes(level))
                h = sha256(h + canonical_bytes(level))
            
            right_label_hex = current.right.label.hex() if (current.right and current.right.label) else ""

            include_gamma = random.random() < GAMMA_PROBABILITY

            if level > 0:
                proof.append({
                    "level": level,
                    "rank": current.rank,
                    "right": right_label_hex,
                    "has_gamma": include_gamma
                })
                if include_gamma:
                    gammas.append(level)
            if current.down:
                current = current.down

        # Locate leaf
        leaf = current
        while remaining > 1 and leaf.right:
            remaining -= 1
            leaf = leaf.right
        
        # Compute leaf hash if not present
        if leaf.leaf_hash is None:
            leaf.leaf_hash = self._hash_leaf(leaf.key, leaf.value)
        
        proof.append({
            "level": 0,
            "key": leaf.key,
            "value": leaf.value,
            "leaf": leaf.leaf_hash.hex(),
            "has_gamma": False
        })

        return leaf.leaf_hash.hex(), proof, gammas


def verify_chain(root_hash, leaf_val, proof):
    if not proof:
        return False, 0, 0

    leaf_entry = proof[-1]
    
    # Count actual proof data (what's stored/transmitted)
    total_bytes = 0
    total_bytes += len(canonical_bytes(leaf_entry["key"]))
    total_bytes += len(canonical_bytes(leaf_entry["value"]))
    total_bytes += 32  # leaf_hash (32 bytes for SHA-256)
    
    gamma_count = 0

    # Compute verification hash starting from leaf
    h = sha256(canonical_bytes(leaf_entry["key"]) + canonical_bytes(leaf_entry["value"]))

    for entry in reversed(proof[:-1]):
        # Count what's actually in the proof structure
        total_bytes += len(canonical_bytes(entry["level"]))
        total_bytes += len(canonical_bytes(entry["rank"]))
        
        right_bytes = bytes.fromhex(entry.get("right", "")) if entry.get("right") else b''
        total_bytes += len(canonical_bytes(right_bytes))
        
        # Count gamma storage
        if entry.get("has_gamma", False):
            gamma_count += 1
            total_bytes += 32  # gamma hash storage (32 bytes)
        
        # Now perform verification hash computation
        ser = canonical_bytes(entry["level"]) + canonical_bytes(entry["rank"]) + canonical_bytes(h) + canonical_bytes(right_bytes)
        h = sha256(ser)

    ok = (h == root_hash)
    return ok, total_bytes, gamma_count


def get_diverse_samples(N, num_samples=100):
    """Generate samples that guarantee crossing byte-encoding boundaries."""
    samples = set()
    
    # CRITICAL: Always include byte-boundary crossing ranks (only if <= N)
    boundaries = [10, 100, 254, 255, 256, 257, 1000, 
                  65534, 65535, 65536, 65537, 100000]
    for b in boundaries:
        if 1 <= b <= N:
            samples.add(b)
    
    # Add logarithmic spacing to hit different encoding sizes
    for i in range(1, 21):  # Up to 2^20
        val = 2**i
        if 1 <= val <= N:
            samples.add(val)
        if 1 <= val - 1 <= N:
            samples.add(val - 1)
        if 1 <= val + 1 <= N:
            samples.add(val + 1)
    
    # Add percentage-based samples across full range
    for p in range(1, 100):  # Every 1% from 1% to 99%
        rank = max(1, min(N, int(N * p / 100)))
        if 1 <= rank <= N:
            samples.add(rank)
    
    # Add specific ranges that cross boundaries (only if applicable)
    if N >= 500:
        for r in range(250, min(270, N + 1)):  # Around 255/256 boundary
            if 1 <= r <= N:
                samples.add(r)
    
    if N >= 70000:
        for r in range(65530, min(65545, N + 1)):  # Around 65535/65536 boundary
            if 1 <= r <= N:
                samples.add(r)
    
    # Fill remaining with random samples
    while len(samples) < min(num_samples, N):
        r = random.randint(1, N)
        samples.add(r)
    
    # Convert to sorted list and ensure all values are valid
    result = sorted([s for s in samples if 1 <= s <= N])[:num_samples]
    
    return result


# Demo

if __name__ == "__main__":
    if RANDOM_SEED is not None:
        random.seed(RANDOM_SEED)
    else:
        random.seed()

    asl = AuthSkipList(expected_size=N)

    keys = list(range(1000, 1000 + N))
    vals = list(range(1000, 1000 + N))

    t0_total = time.perf_counter()
    for i, (k, v) in enumerate(zip(keys, vals), start=1):
        asl.insert(k, v)
    t1_total = time.perf_counter()

    root = asl.get_root()
    print("Root hash:", root.hex())

    csv_filename = f"asl_results_{N}nodes.csv"

    with open(csv_filename, "w", newline="") as f:
        writer = csv.DictWriter(f, fieldnames=[
            "Node #", "Key", "Proof Computation Time (µs)",
            "Proof Verification Time (µs)",
            "Proof Gamma Storage (bytes)", "Proof Serialized Bytes",
            "Proof Length (levels)", "Gamma Count"
        ])
        writer.writeheader()

        # Get diverse samples that cross encoding boundaries
        sample_positions = get_diverse_samples(N, num_samples=N // SAMPLE_EVERY)

        # Comp. and verify timer
        for i in sample_positions:
            t0 = time.perf_counter()
            leaf_val, proof, gamma_levels = asl.get_proof_chain(i)
            t1 = time.perf_counter()
            proof_time = (t1 - t0) * 1e6

            t2 = time.perf_counter()
            ok, nbytes, gamma_count = verify_chain(root, leaf_val, proof)
            t3 = time.perf_counter()
            verify_time = (t3 - t2) * 1e6

            proof_length = len(proof)
            gamma_storage = gamma_count * 32

            writer.writerow({
                "Node #": i,
                "Key": proof[-1]["key"],
                "Proof Computation Time (µs)": proof_time,
                "Proof Verification Time (µs)": verify_time,
                "Proof Gamma Storage (bytes)": gamma_storage,
                "Proof Serialized Bytes": nbytes,
                "Proof Length (levels)": proof_length,
                "Gamma Count": gamma_count
            })

    df = pd.read_csv(csv_filename)

    summary = pd.DataFrame({
        "Metric": [
            "Average Proof Computation Time (µs)",
            "Average Proof Verification Time (µs)",
            "Average Gamma Storage (bytes)",
            "Average Proof Serialized Bytes",
            "",
            "Largest Proof Computation Time (µs)",
            "Largest Proof Verification Time (µs)",
            "Largest Gamma Storage (bytes)",
            "Largest Proof Serialized Bytes",
            "",
            "Std Dev Proof Computation Time (µs)",
            "Std Dev Proof Verification Time (µs)",
            "Std Dev Gamma Storage (bytes)",
            "Std Dev Proof Serialized Bytes",
        ],
        "Value": [
            round(df["Proof Computation Time (µs)"].mean(), 2),
            round(df["Proof Verification Time (µs)"].mean(), 2),
            round(df["Proof Gamma Storage (bytes)"].mean(), 2),
            round(df["Proof Serialized Bytes"].mean(), 2),
            "",
            round(df["Proof Computation Time (µs)"].max(), 2),
            round(df["Proof Verification Time (µs)"].max(), 2),
            round(df["Proof Gamma Storage (bytes)"].max(), 2),
            round(df["Proof Serialized Bytes"].max(), 2),
            "",
            round(df["Proof Computation Time (µs)"].std(), 2),
            round(df["Proof Verification Time (µs)"].std(), 2),
            round(df["Proof Gamma Storage (bytes)"].std(), 2),
            round(df["Proof Serialized Bytes"].std(), 2),
        ]
    })

    summary.to_csv(f"Results_{N}nodes.csv", index=False)
    print(f"=== Proof Performance Summary ({N} nodes) ===")
    print(summary.to_string(index=False, justify="center", float_format=lambda x: f"{x:.2f}"))

#!/usr/bin/env python3
import hashlib, random, struct, time, csv, math
import pandas as pd
import argparse

# CLI --size #
parser = argparse.ArgumentParser(description="Benchmark")
parser.add_argument(
    "--size", "-s",
    type=int,
    default=4096,
    help="Number of nodes to insert (e.g., 1024, 4096, 1048576)"
)
args = parser.parse_args()
N = args.size

# Configs
BATCH_RECOMPUTE = 512
SAMPLE_EVERY = 10
RANDOM_SEED = None
GAMMA_PROBABILITY = 0.5  # Probability of including gamma at each level


def sha256(x: bytes) -> bytes:
    return hashlib.sha256(x).digest()

def canonical_bytes(arg):
    if isinstance(arg, int):
        data = b'\x00' if arg == 0 else arg.to_bytes((arg.bit_length() + 7) // 8, 'big')
        tag = b'I'
    elif isinstance(arg, str):
        data = arg.encode('utf-8')
        tag = b'S'
    elif isinstance(arg, bytes):
        data = arg
        tag = b'B'
    elif arg is None:
        data = b''
        tag = b'N'
    else:
        data = str(arg).encode('utf-8')
        tag = b'S'
    return tag + struct.pack('>I', len(data)) + data


class Node:
    __slots__ = ('level', 'key', 'value', 'down', 'right', 'rank', 'label', 'leaf_hash')
    def __init__(self, level, key=None, value=None):
        self.level = level
        self.key = key
        self.value = value
        self.down = None
        self.right = None
        self.rank = 0
        self.label = None
        self.leaf_hash = None


class AuthSkipList:
    def __init__(self, expected_size):
        self.max_level = math.ceil(math.log2(expected_size)) if expected_size > 0 else 8
        self.level_heads = [Node(l) for l in range(self.max_level + 1)]
        for l in range(self.max_level, 0, -1):
            self.level_heads[l].down = self.level_heads[l - 1]
        self.head = self.level_heads[self.max_level]
        self.count = 0
        self.actual_height = 0
        # Initialize all head labels to empty hash
        for head in self.level_heads:
            head.label = sha256(canonical_bytes(head.level) + canonical_bytes(0) + 
                               canonical_bytes(b'') + canonical_bytes(b''))

    def _hash_leaf(self, key, value):
        return sha256(canonical_bytes(key) + canonical_bytes(value))

    def random_level(self):
        lvl = 0
        while random.random() < 0.5 and lvl < self.max_level:
            lvl += 1
        return lvl

    def _compute_rank(self, node):
        """Compute rank of a node from its down pointer."""
        if node.level == 0:
            return 1
        subtotal = 0
        d = node.down
        while d:
            subtotal += d.rank
            d = d.right
        return subtotal

    def _compute_label(self, node):
        """Compute label of a node from its children."""
        right_hash = node.right.label if (node.right and node.right.label) else b''
        if node.level == 0:
            child_hash = node.leaf_hash if node.leaf_hash else b''
        else:
            child_hash = node.down.label if (node.down and node.down.label) else b''
        ser = canonical_bytes(node.level) + canonical_bytes(node.rank) \
              + canonical_bytes(child_hash) + canonical_bytes(right_hash)
        return sha256(ser)

    def insert(self, key, value):
        prevs = [None] * (self.max_level + 1)
        node = self.head
        
        # Find insertion path
        for l in reversed(range(self.max_level + 1)):
            while node.right and node.right.key is not None and node.right.key < key:
                node = node.right
            prevs[l] = node
            if node.down:
                node = node.down

        lvl = self.random_level()
        if lvl > self.actual_height:
            self.actual_height = lvl

        down_link = None
        new_nodes = []
        for l in range(lvl + 1):
            new = Node(l, key, value)
            if l == 0:
                new.leaf_hash = self._hash_leaf(key, value)
                new.rank = 1
                new.label = new.leaf_hash
            new.down = down_link
            new.right = prevs[l].right
            prevs[l].right = new
            new_nodes.append(new)
            down_link = new
        self.count += 1

        self._update_path(prevs, new_nodes)

    def _update_path(self, prevs, new_nodes):
        """Update ranks and labels along the insert. path only."""
        
        # Update ranks bottom-up for new nodes
        for l in range(len(new_nodes)):
            new_nodes[l].rank = self._compute_rank(new_nodes[l])
        
        # Update ranks for nodes on the insert. path
        for l in range(len(prevs)):
            if prevs[l] is None:
                continue
            prevs[l].rank = self._compute_rank(prevs[l])
        
        # Must update labels from bottom to top so child labels are ready
        for l in range(self.actual_height + 1):

            if l < len(new_nodes):
                new_nodes[l].label = self._compute_label(new_nodes[l])
            
            # Update predecessor node label
            if l < len(prevs) and prevs[l]:
                prevs[l].label = self._compute_label(prevs[l])
            
            # Update head node at this level
            head = self.level_heads[l]
            head.rank = self._compute_rank(head)
            head.label = self._compute_label(head)
        
        # Update head nodes above actual_height up to max_level
        for l in range(self.actual_height + 1, self.max_level + 1):
            head = self.level_heads[l]
            head.rank = self._compute_rank(head)
            head.label = self._compute_label(head)

    def get_root(self):
        return self.head.label

    def get_actual_height(self):
        return self.actual_height

    def get_proof_chain(self, rank):
        if rank < 1 or rank > self.count:
            raise IndexError("rank out of range")

        proof = []
        gammas = []  
        remaining = rank
        current = self.head

        for level in reversed(range(self.max_level + 1)):
            while current.right and current.right.rank < remaining:
                remaining -= current.right.rank
                current = current.right
            right_label_hex = current.right.label.hex() if (current.right and current.right.label) else ""
            
            include_gamma = random.random() < GAMMA_PROBABILITY
            
            if level > 0:
                proof.append({
                    "level": level,
                    "rank": current.rank,
                    "right": right_label_hex,
                    "has_gamma": include_gamma
                })
                if include_gamma:
                    gammas.append(level)
            if current.down:
                current = current.down

        # Locate leaf
        leaf = current
        while remaining > 1 and leaf.right:
            remaining -= 1
            leaf = leaf.right
        proof.append({
            "level": 0,
            "key": leaf.key,
            "value": leaf.value,
            "leaf": leaf.leaf_hash.hex(),
            "has_gamma": False
        })

        return leaf.leaf_hash.hex(), proof, gammas


def verify_chain(root_hash, leaf_val, proof):
    if not proof:
        return False, 0, []

    leaf_entry = proof[-1]
    h = sha256(canonical_bytes(leaf_entry["key"]) + canonical_bytes(leaf_entry["value"]))
    total_bytes = len(canonical_bytes(leaf_entry["key"]) + canonical_bytes(leaf_entry["value"]))
    gamma_count = 0

    for entry in reversed(proof[:-1]):
        right_bytes = bytes.fromhex(entry.get("right", "")) if entry.get("right") else b''
        rank = entry.get("rank", 1)
        ser = canonical_bytes(entry["level"]) + canonical_bytes(rank) + canonical_bytes(h) + canonical_bytes(right_bytes)
        h = sha256(ser)
        total_bytes += len(ser)
        
        # Count gamma storage
        if entry.get("has_gamma", False):
            gamma_count += 1

    ok = (h == root_hash)
    return ok, total_bytes, gamma_count

# Demo

if __name__ == "__main__":
    if RANDOM_SEED is not None:
        random.seed(RANDOM_SEED)
    else:
        random.seed()

    asl = AuthSkipList(expected_size=N)

    keys = list(range(1000, 1000 + N))
    vals = list(range(1000, 1000 + N))

    print(f"Inserting {N} elements...")
    t0_total = time.perf_counter()
    for i, (k, v) in enumerate(zip(keys, vals), start=1):
        asl.insert(k, v)
    t1_total = time.perf_counter()
    
    root = asl.get_root()
    print("Root hash:", root.hex())

    csv_filename = f"asl_results_{N}nodes.csv"
    
    with open(csv_filename, "w", newline="") as f:
        writer = csv.DictWriter(f, fieldnames=[
            "Node #", "Key", "Proof Computation Time (µs)",
            "Proof Verification Time (µs)",
            "Proof Gamma Storage (bytes)", "Proof Serialized Bytes",
            "Proof Length (levels)", "Gamma Count"
        ])
        writer.writeheader()

        # Comp. and verify timer
        for i in range(1, N + 1):
            if i % SAMPLE_EVERY != 0:
                continue
            t0 = time.perf_counter()
            leaf_val, proof, gamma_levels = asl.get_proof_chain(i)
            t1 = time.perf_counter()
            proof_time = (t1 - t0) * 1e6

            t2 = time.perf_counter()
            ok, nbytes, gamma_count = verify_chain(root, leaf_val, proof)
            t3 = time.perf_counter()
            verify_time = (t3 - t2) * 1e6

            proof_length = len(proof)
            gamma_storage = gamma_count * 32  

            writer.writerow({
                "Node #": i,
                "Key": proof[-1]["key"],
                "Proof Computation Time (µs)": proof_time,
                "Proof Verification Time (µs)": verify_time,
                "Proof Gamma Storage (bytes)": gamma_storage,
                "Proof Serialized Bytes": nbytes,
                "Proof Length (levels)": proof_length,
                "Gamma Count": gamma_count
            })

    df = pd.read_csv(csv_filename)

    summary = pd.DataFrame({
        "Metric": [
            "Average Proof Computation Time (µs)",
            "Average Proof Verification Time (µs)",
            "Average Gamma Storage (bytes)",
            "Average Proof Serialized Bytes",
            "",
            "Largest Proof Computation Time (µs)",
            "Largest Proof Verification Time (µs)",
            "Largest Gamma Storage (bytes)",
            "Largest Proof Serialized Bytes",
            "",
            "Std Dev Proof Computation Time (µs)",
            "Std Dev Proof Verification Time (µs)",
            "Std Dev Gamma Storage (bytes)",
            "Std Dev Proof Serialized Bytes",
        ],
        "Value": [
            round(df["Proof Computation Time (µs)"].mean(), 2),
            round(df["Proof Verification Time (µs)"].mean(), 2),
            round(df["Proof Gamma Storage (bytes)"].mean(), 2),
            round(df["Proof Serialized Bytes"].mean(), 2),
            "",
            round(df["Proof Computation Time (µs)"].max(), 2),
            round(df["Proof Verification Time (µs)"].max(), 2),
            round(df["Proof Gamma Storage (bytes)"].max(), 2),
            round(df["Proof Serialized Bytes"].max(), 2),
            "",
            round(df["Proof Computation Time (µs)"].std(), 2),
            round(df["Proof Verification Time (µs)"].std(), 2),
            round(df["Proof Gamma Storage (bytes)"].std(), 2),
            round(df["Proof Serialized Bytes"].std(), 2),
        ]
    })

    summary.to_csv(f"Results_{N}nodes.csv", index=False)
    print(f"=== Proof Performance Summary ({N} nodes) ===")
    print(summary.to_string(index=False, justify="center", float_format=lambda x: f"{x:.2f}"))
